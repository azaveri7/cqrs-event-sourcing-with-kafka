===============
Docker
===============

docker --version

docker ps -a

docker network create --attachable -d bridge techbankNet

docker network ls

docker-compose --version

===============
Kafka
===============

Note: use the docker-compose.yml present in root directory and not the one in Materials folder.

The one in the Materials folder mention latest tag for Kafka, and latest Kafka version does not use zookeeper



docker-compose up -d

The above command install kafka broker and zookeeper listed in docker-compose file

===============
MongoDB setup
===============

docker run -it -d --name mongo-container -p 27017:27017 --network techbankNet --restart always -v mongo_data_container:/data/db mongo:latest

===============
MySQL setup
===============

docker run -it -d --name mysql-container -p 3306:3306 --network techbankNet -e MYSQL_ROOT_PASSWORD=techbankRootPwd --restart always -v mysql__data_container:/var/lib/mysql mysql:latest

docker run -it -d --name adminer -p 8080:8080 --network techbankNet -e ADMINER_DEFAULT_SERVER=mysql-container --restart always adminer:latest


=================
Mediator Pattern
=================

Behavioral design Pattern

Promotes loose coupling by preventing objects from referring to each other explicitly.

Simplifies communication between objects by introducing a single object known as the
mediator that manages the distribution of messages among other objects.

=================
Aggregate
=================

An Aggregate is an entity or group  of entities that is always kept in  a consistent state.

The Aggregate root is the entity within the Aggregate that is responsible for maintaining
this consistent state.

This makes the Aggregate the primary building block for implementing a command model in any
CQRS based application.

=================
Event store
=================

An event store is a database that is used to store data as a sequence of immutable events
over time.

An event store must be an append only store, no update or delete operations should be allowed.

Each event that is saved should represent the version or state of an aggregate at any given point
in time.

Events should be stored in chronological order, and new events should be appended to the previous
event.

The state of the aggregate should be recreatable by replaying the event store.

Implement optimistic concurrency control

Event sourcing is based on building the state of the aggregate based on the order of the sequence
of events. For the state to be correct, it is important that the ordering of events is enforced
by implementing event versioning. Optimistic concurrency control is then used to ensure that only the
expected event versions can alter the state of the aggregate at any given point in time. This is in
especially important when two or more clients requests are made at the same time to alter the state of
the aggregate.

=================
Domain driven design
=================

An approach to structure and model software in a way that it matches the business domain.

Refers to problems as domains and aims to establish a common language to talk about these problems

Describes independent problem areas as Bounded Contexts

Bounded context - is an independent problem area.

Describes a logical boundary within which a particular model is defined and applicable.

Each bounded context correlates to a microservice. (e.g. BankAccount Microserve)

====================
Important notes:
====================
The responsibility for retrieving events from the Event Store and invoking
the replayEvents method lies with the EventSourcingHandler, not the EventHandler,
which operates on the read or query side.

This distinction is crucial for understanding how different components work in an
event-sourced architecture.

The EventHandler indeed updates the read database via the AccountRepository after
consuming an event, which is essential for maintaining the accuracy of the read model
in an event-sourced architecture.

This understanding is key to seeing how events are processed and data is synchronized
in such systems.

====================
Kafka consumer
====================

Kafka tracks a separate consumer offset for each consumer group to allow
distinct groups of consumers to independently manage their reading progress from a topic.

This design enables effective scaling and load balancing among consumers within the same group
while still ensuring that every group can track its own offset without interfering with others.

The order of events in Kafka is indeed critical; it ensures that events are processed
in the sequence they occur, which is essential for maintaining consistency
and accuracy in event-sourced systems.
